Позволяет построить математическую модель, наилучшим образом соответствующую набору данных, полученных в ходе машинного эксперимента.
Под наилучшим соответствием понимается минимизированная функция ошибки, характеризующая различие между прогнозируемой моделью и данными эксперимента.
Такой функцией в регрессионном анализе является сумма квадратов отклонений экспериментальных значений от прогнозируемых – метод наименьших квадратов.
Пусть исследуется зависимость некоторой величины у от величины х.
Зависимость у от х предполагается описывать с помощью модели у = φ(х).
Вид функциональной зависимости (линейная, квадратичная, экспоненциальная и т. д.) может быть выбран исходя из априорных сведений об исследуемой системе; характера расположения экспериментальных точек на плоскости.
По результатам машинного эксперимента требуется установить параметры этой зависимости.
Пусть в результате машинного эксперимента получены точки (xi, yi), i = 1, 2, …, N. Обозначим числовые параметры функции φ через a, b, c, …
Метод наименьших квадратов:
Параметры a, b, c,… следует выбрать так, чтобы ∑_(i=1)^N▒〖(y_i-φ(x_i,a,b,c,…))〗^2 →min.
εi – ошибка i-й экспериментальной точки 




Для отыскания искомых значений a, b, c, … – приравнять к нулю частные производные минимизируемой функции по аргументам a, b, c, … :


{█(∑_(i=1)^N▒〖├ [y_i-φ(x_i,a,b,c,…)]*(φ/a)┤|_((x_i,y_i))=0〗@∑_(i=1)^N▒〖├ [y_i-φ(x_i,a,b,c,…)]*(φ/b)┤|_((x_i,y_i))=0〗@∑_(i=1)^N▒〖├ [y_i-φ(x_i,a,b,c,…)]*(φ/c)┤|_((x_i,y_i))=0〗)┤    (*)
Примеры.
1. Построение линейной регрессионной модели.
Прогнозируемая модель y=ax+b (φ(x,a,b)=ax+b), тогда εi = yi – (axi + b).
Тогда εi = yi – (axi + b) и система (*) имеет вид:
{█(∑_(i=1)^N▒〖[y_i-(ax_i+b)]*x_i=0〗@∑_(i=1)^N▒〖[y_i-(ax_i+b)]*1〗=0)┤


Или {█(a∑_(i=1)^N▒〖x_i^2+b∑_(i=1)^N▒〖x_i=∑_(i=1)^N▒〖x_i y_i 〗〗〗@a∑_(i=1)^N▒〖x_i+bN=∑_(i=1)^N▒y_i 〗)┤
Искомые значения: a=(N∑_(i=1)^N▒〖x_i y_i-∑_(i=1)^N▒x_i *∑_(i=1)^N▒y_i 〗)/(N∑_(i=1)^N▒x_i^2 -〖(∑_(i=1)^N▒x_i )〗^2 ),b=  (∑_(i=1)^N▒x_i^2 *∑_(i=1)^N▒〖y_i-∑_(i=1)^N▒〖x_i*∑_(i=1)^N▒〖x_i y_i 〗〗〗)/(N∑_(i=1)^N▒〖x_i^2-〖(∑_(i=1)^N▒x_i )〗^2 〗)
2. Построение квадратичной регрессионной модели.
Прогнозируемая модель
y=ax2+bx+c (φ(x,a,b)=ax2+bx+c)
Тогда εi = yi – (axi2 + bxi+c) и система (*) имеет вид:
{█(∑_(i=1)^N▒〖[y_i-(〖ax〗_i^2+bx_i+c)] x_i^2=0〗@∑_(i=1)^N▒〖[y_i-(〖ax〗_i^2+bx_i+c)] x_i=0〗@∑_(i=1)^N▒〖[y_i-(〖ax〗_i^2+bx_i+c)]1=0〗)┤
Или
{█(a∑_(i=1)^N▒x_i^4 +b∑_(i=1)^N▒x_i^3 +c∑_(i=1)^N▒x_i^2 =∑_(i=1)^N▒〖x_i^2 y_i 〗@a∑_(i=1)^N▒x_i^3 +b∑_(i=1)^N▒x_i^3 +c∑_(i=1)^N▒x_i^  =∑_(i=1)^N▒〖x_i^  y_i 〗@a∑_(i=1)^N▒x_i^2 +b∑_(i=1)^N▒x_i^  +cN=∑_(i=1)^N▒y_i )┤
Искомые значения – решение полученной системы. В обоих случаях коэффициенты при неизвестных в полученных системах уравнений – статистические моменты системы СВ Х и Y, умноженные на N. 
